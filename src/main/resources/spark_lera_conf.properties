# Spark Configs

#(spark.app.name) => Spark application name
spark.app.name=lera_loader

# log_level options INFO, DEBUG, OFF, WARN, FATAL
spark.log_level=INFO

# Environment info

# Configuration DB names
spark.lera_audit_database=lera_etl
spark.lera_config_database=lera_config
# Configuration tables
spark.lera_audit=etl_audit
spark.lera_config=etl_generic_config
spark.lera_column_mapping_table=etl_column_mapping
spark.lera_default_values_table=etl_default_values
spark.lera_join_table=etl_join_table_mapping
spark.lera_delete_table=etl_delete_data
spark.lera_filter_table=etl_filter_data
spark.lera_partition_database=etl_partition_column
spark.lera_generic_lookup_table=etl_generic_lookup

# JDBC Connection Info
#The below properties should be used when using impala driver to connect
#Currently in this project, we are using hive jdbc driver to connect

#spark.impala_username=user@AP.CORP.LERA.COM
#spark.impala_password=Cars@123!
#spark.impalaConnectionURL=jdbc:impala://cluster-impala.org.com:21050
#spark.impalaJDBCDriver=com.cloudera.impala.jdbc41.driver

spark.hive_username=administrator
spark.hive_password=Welcome12345
spark.hiveConnectionURL=jdbc:hive2://node1.lera.us:2181
spark.hiveJDBCDriver=org.apache.hive.jdbc.HiveDriver

# Applicable for column mapping
spark.skip_missing_source_column_source_systems=
# Applicable for column mapping and default values
spark.skip_missing_target_column_source_systems=
spark.excel_sheet_name=Sheet1
spark.kudu_default_partitions=200
spark.read_kudu_using_hive_source_systems=

#Set the source system transformer to hive as below and this will use hive writer to write the data
#spark.agrosoft_inventory_writers=hive

spark.smtp_host=
spark.lera_notification_mail_list=
spark.lera_notification_mail_subject=
#Run time should be specified in minutes
spark.lera_default_max_run_time=120
spark.lera_default_notification_interval_time=30
spark.job_exec_control_table=job_execution_control

# Transformation based on source system
#spark.lera_condition_table=lera_cascna_jde_lookup_mapping
#spark.lera_generic_config_transformers=TypeCast
#spark.rtduet_transformers=ColumnMap,DefaultValue,RtDuet,Joiner,TypeCast
#spark.jupiter_oc_transformers=ColumnMap,DefaultValue,Jupiter_oc,Joiner,TypeCast
##spark.jupiter_ld_transformers=ColumnMap,DefaultValue,Jupiter_ld,Joiner,TypeCast
#spark.magellan_transformers=ColumnMap,DefaultValue,Magellan,Joiner,Delete,TypeCast
#spark.open_contracts_transformers=Delete,Joiner,Lynx_open_contracts,ColumnMap,DefaultValue
##spark.agrosoft_transformers=Delete,ColumnMap,DefaultValue,Joiner,Agrosoft,TypeCast
#spark.lynx_inventory_transformers=Joiner,Lynx_Inventory,ColumnMap,DefaultValue,TypeCast
#spark.agrosoft_inventory_transformers=ColumnMap,DefaultValue,Agrosoft_Inventory,TypeCast
#spark.lynx_commodity_transformers=ColumnMap,DefaultValue,TypeCast
#spark.jde_volume_transformers=JDEVolume,ColumnMap,DefaultValue,TypeCast
#spark.lynx_commodity_movement_transformers=ColumnMap,DefaultValue,Lynx_Commodity_Movement
#spark.lynx_facilities_transformers=ColumnMap,DefaultValue,TypeCast
#spark.lynx_cash_position_stg_transformers=ColumnMap,DefaultValue,Filter,TypeCast
#spark.lynx_cash_position_transformers=ColumnMap,DefaultValue,Lynx_Cash_Position,TypeCast
#spark.towworks_cp_stg1_transformers=TowWorksstg,TypeCast
#spark.towworks_cp_stg2_transformers=TowWorksstg,TypeCast
#spark.towworks_cp_transformers=Filter,TowWorks,ColumnMap,DefaultValue,TypeCast
#spark.macbeth_transformers=ColumnMap,DefaultValue,MacBeth,TypeCast
#spark.towworks_barges_stg1_transformers=TowWorksstg,TypeCast
#spark.towworks_barges_stg2_transformers=TowWorksstg,TypeCast
#spark.towworks_barges_transformers=DefaultValue,ColumnMap,TowWorksBarges,TypeCast
#spark.towworks_contracts_position_transformers=TowWorksContracts,ColumnMap,DefaultValue
#spark.agrosoft_commodity_transformers=ColumnMap,DefaultValue,TypeCast
#spark.jde_commodity_movements_transformers=ColumnMap,TypeCast
#spark.lynx_org_organization_transformers=ColumnMap,DefaultValue,TypeCast
#spark.agrosoft_org_organization_transformers=ColumnMap,DefaultValue,TypeCast
#spark.voyages_stg1_transformers=ColumnMap,DefaultValue,BSMVoyagesstg,TypeCast

# Transformation based on filter or delete condition from Source to Staging Ingestion
#spark.lynx_inventory_stg_transformers=ColumnMap,DefaultValue,Filter,TypeCast
#spark.agrosoft_inventory_stg_transformers=ColumnMap,DefaultValue,Delete,TypeCast
#spark.jde_finance_transformers=JDE,ColumnMap,DefaultValue,TypeCast
#spark.facility_transformers=ColumnMap,DefaultValue,Facility,TypeCast
#spark.agrosoft_contracts_transformers=Joiner,LOOKUP,Agrosoft_contracts,ColumnMap,DefaultValue
#spark.trip_status_stg2_transformers=Trip_status_stg2
#spark.trip_status_transformers=Joiner,Trip_status,ColumnMap,DefaultValue,TypeCast
#spark.lynx_oc_transformers=Delete,Joiner,Lynx_oc,ColumnMap,DefaultValue,TypeCast
#spark.magellan_exp_projections_transformers=Joiner,ColumnMap,DefaultValue,magellan_exp_projection
#spark.JDE_Inventory_transformers=TypeCast
#spark.JDE_Inventory2_transformers=JDE_Inventory2,TypeCast
#spark.lynx_inventory_writers=hive
#spark.agrosoft_inventory_writers=hive
#spark.truncate_target_based_source_systems=trip_status,agrosoft,open_contracts,agrosoft

#spark.jde_contracts_stg_transformers=Filter
#spark.jde_contracts_transformers=Joiner,Jde_contracts,ColumnMap,DefaultValue,TypeCast

